{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import joblib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ü§ñ Model Experiments and Comparison\")\n",
    "print(f\"üìÖ Started at: {datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c16c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "print(\"üìÇ Loading Processed Data...\")\n",
    "\n",
    "try:\n",
    "    # Try to load pre-processed data\n",
    "    processed_dir = Path('../data/processed')\n",
    "    \n",
    "    df_processed = pd.read_csv(processed_dir / 'processed_data.csv')\n",
    "    print(f\"‚úÖ Processed data loaded: {df_processed.shape}\")\n",
    "    \n",
    "    # Load train-test splits if available\n",
    "    if (processed_dir / 'X_train.csv').exists():\n",
    "        X_train = pd.read_csv(processed_dir / 'X_train.csv')\n",
    "        X_test = pd.read_csv(processed_dir / 'X_test.csv')\n",
    "        y_train = pd.read_csv(processed_dir / 'y_train.csv')['price']\n",
    "        y_test = pd.read_csv(processed_dir / 'y_test.csv')['price']\n",
    "        \n",
    "        print(f\"‚úÖ Pre-split data loaded:\")\n",
    "        print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        data_loaded = True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Pre-split data not found, will create splits\")\n",
    "        data_loaded = False\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Processed data not found. Loading raw data and processing...\")\n",
    "    \n",
    "    # Load raw data and do basic processing\n",
    "    try:\n",
    "        df_raw = pd.read_csv('../data/raw/bengaluru_house_prices.csv')\n",
    "    except FileNotFoundError:\n",
    "        # Create sample data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 2000\n",
    "        locations = ['Whitefield', 'Electronic City', 'Marathahalli', 'BTM Layout', 'Koramangala', \n",
    "                    'HSR Layout', 'Indiranagar', 'Jayanagar']\n",
    "        \n",
    "        df_raw = pd.DataFrame({\n",
    "            'location': np.random.choice(locations, n_samples),\n",
    "            'total_sqft': np.random.normal(1200, 400, n_samples),\n",
    "            'size': [f\"{np.random.choice([1,2,3,4])} BHK\" for _ in range(n_samples)],\n",
    "            'bath': np.random.randint(1, 5, n_samples),\n",
    "            'balcony': np.random.randint(0, 4, n_samples),\n",
    "            'price': np.random.normal(80, 30, n_samples)\n",
    "        })\n",
    "        df_raw['total_sqft'] = np.maximum(df_raw['total_sqft'], 500)\n",
    "        df_raw['price'] = np.maximum(df_raw['price'], 20)\n",
    "        \n",
    "        print(\"üìä Sample data created for experiments\")\n",
    "    \n",
    "    # Quick processing\n",
    "    df_processed = df_raw.copy()\n",
    "    \n",
    "    # Extract BHK from size\n",
    "    df_processed['bhk'] = df_processed['size'].str.extract('(\\d+)').astype(int)\n",
    "    \n",
    "    # Fill missing balcony\n",
    "    df_processed['balcony'] = df_processed['balcony'].fillna(0)\n",
    "    \n",
    "    data_loaded = False\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"Shape: {df_processed.shape}\")\n",
    "print(f\"Columns: {list(df_processed.columns)}\")\n",
    "display(df_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a39d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"üîß Preparing Data for Modeling...\")\n",
    "\n",
    "if not data_loaded:\n",
    "    # Create features for modeling\n",
    "    feature_columns = []\n",
    "    \n",
    "    # Numerical features\n",
    "    numerical_features = ['total_sqft', 'bhk', 'bath', 'balcony']\n",
    "    feature_columns.extend([col for col in numerical_features if col in df_processed.columns])\n",
    "    \n",
    "    # Encode location if present\n",
    "    if 'location' in df_processed.columns:\n",
    "        le_location = LabelEncoder()\n",
    "        df_processed['location_encoded'] = le_location.fit_transform(df_processed['location'])\n",
    "        feature_columns.append('location_encoded')\n",
    "    \n",
    "    # Create derived features\n",
    "    if 'total_sqft' in df_processed.columns and 'bhk' in df_processed.columns:\n",
    "        df_processed['sqft_per_room'] = df_processed['total_sqft'] / df_processed['bhk']\n",
    "        feature_columns.append('sqft_per_room')\n",
    "    \n",
    "    if 'bath' in df_processed.columns and 'bhk' in df_processed.columns:\n",
    "        df_processed['bath_per_bhk'] = df_processed['bath'] / df_processed['bhk']\n",
    "        feature_columns.append('bath_per_bhk')\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df_processed[feature_columns].copy()\n",
    "    y = df_processed['price'].copy()\n",
    "    \n",
    "    # Handle any missing values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data preparation completed\")\n",
    "    print(f\"   Features: {feature_columns}\")\n",
    "\n",
    "print(f\"\\nüìä Final Data for Modeling:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Feature columns: {list(X_train.columns)}\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nüìà Feature Statistics:\")\n",
    "display(X_train.describe().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to experiment with\n",
    "print(\"üéØ Configuring Models for Experiments...\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    \n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    \n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    \n",
    "    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    \n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    \n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Extra Trees': ExtraTreesRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    \n",
    "    'Support Vector Regression': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(models)} models for comparison:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   ‚Ä¢ {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97640f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation evaluation of all models\n",
    "print(\"üìä Cross-Validation Evaluation...\")\n",
    "\n",
    "def evaluate_model_cv(model, X, y, cv_folds=5, scoring='r2'):\n",
    "    \"\"\"Evaluate model using cross-validation\"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv=cv_folds, scoring=scoring)\n",
    "    return {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "cv_results = {}\n",
    "cv_folds = 5\n",
    "\n",
    "print(f\"Running {cv_folds}-fold cross-validation for all models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # R¬≤ score\n",
    "        r2_results = evaluate_model_cv(model, X_train, y_train, cv_folds, 'r2')\n",
    "        \n",
    "        # MAE score\n",
    "        mae_results = evaluate_model_cv(model, X_train, y_train, cv_folds, 'neg_mean_absolute_error')\n",
    "        mae_results['mean'] = -mae_results['mean']  # Convert back to positive\n",
    "        mae_results['scores'] = -mae_results['scores']\n",
    "        \n",
    "        # RMSE score\n",
    "        rmse_results = evaluate_model_cv(model, X_train, y_train, cv_folds, 'neg_mean_squared_error')\n",
    "        rmse_results['mean'] = np.sqrt(-rmse_results['mean'])\n",
    "        rmse_results['scores'] = np.sqrt(-rmse_results['scores'])\n",
    "        \n",
    "        cv_results[name] = {\n",
    "            'r2': r2_results,\n",
    "            'mae': mae_results,\n",
    "            'rmse': rmse_results\n",
    "        }\n",
    "        \n",
    "        print(f\"   R¬≤ Score: {r2_results['mean']:.4f} (¬±{r2_results['std']:.4f})\")\n",
    "        print(f\"   MAE: {mae_results['mean']:.2f} (¬±{mae_results['std']:.2f})\")\n",
    "        print(f\"   RMSE: {rmse_results['mean']:.2f} (¬±{rmse_results['std']:.2f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {str(e)}\")\n",
    "        cv_results[name] = None\n",
    "\n",
    "print(f\"\\n‚úÖ Cross-validation completed for {len([k for k, v in cv_results.items() if v is not None])} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1376351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on full training set and evaluate on test set\n",
    "print(\"üöÄ Training Models on Full Dataset...\")\n",
    "\n",
    "# Scale features for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Models that need scaling\n",
    "scaling_models = ['Support Vector Regression', 'K-Nearest Neighbors']\n",
    "\n",
    "# Training and testing results\n",
    "test_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if cv_results.get(name) is None:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Choose scaled or unscaled data\n",
    "        if name in scaling_models:\n",
    "            X_train_use = X_train_scaled_df\n",
    "            X_test_use = X_test_scaled_df\n",
    "        else:\n",
    "            X_train_use = X_train\n",
    "            X_test_use = X_test\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_use, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train_use)\n",
    "        y_test_pred = model.predict(X_test_use)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_test - y_test_pred\n",
    "        \n",
    "        test_results[name] = {\n",
    "            'model': model,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mape': test_mape,\n",
    "            'predictions': y_test_pred,\n",
    "            'residuals': residuals,\n",
    "            'cv_r2_mean': cv_results[name]['r2']['mean'],\n",
    "            'cv_r2_std': cv_results[name]['r2']['std']\n",
    "        }\n",
    "        \n",
    "        print(f\"   Train R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"   Test R¬≤: {test_r2:.4f}\")\n",
    "        print(f\"   Test MAE: {test_mae:.2f}\")\n",
    "        print(f\"   Test RMSE: {test_rmse:.2f}\")\n",
    "        print(f\"   Test MAPE: {test_mape:.4f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        overfitting = train_r2 - test_r2\n",
    "        if overfitting > 0.1:\n",
    "            print(f\"   ‚ö†Ô∏è Potential overfitting detected (difference: {overfitting:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Training failed: {str(e)}\")\n",
    "        test_results[name] = None\n",
    "\n",
    "print(f\"\\n‚úÖ Model training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results comparison\n",
    "print(\"üìä Model Performance Comparison\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_data = []\n",
    "\n",
    "for name, results in test_results.items():\n",
    "    if results is not None:\n",
    "        results_data.append({\n",
    "            'Model': name,\n",
    "            'CV_R2_Mean': results['cv_r2_mean'],\n",
    "            'CV_R2_Std': results['cv_r2_std'],\n",
    "            'Train_R2': results['train_r2'],\n",
    "            'Test_R2': results['test_r2'],\n",
    "            'Test_MAE': results['test_mae'],\n",
    "            'Test_RMSE': results['test_rmse'],\n",
    "            'Test_MAPE': results['test_mape'],\n",
    "            'Overfitting': results['train_r2'] - results['test_r2']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Model Performance Rankings:\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_results = test_results[best_model_name]\n",
    "\n",
    "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "print(f\"   Test R¬≤ Score: {best_model_results['test_r2']:.4f}\")\n",
    "print(f\"   Test MAE: {best_model_results['test_mae']:.2f} lakhs\")\n",
    "print(f\"   Test RMSE: {best_model_results['test_rmse']:.2f} lakhs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "print(\"üìà Creating Performance Visualizations...\")\n",
    "\n",
    "# Set up the plotting\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. R¬≤ Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "model_names = results_df['Model'].values\n",
    "r2_scores = results_df['Test_R2'].values\n",
    "\n",
    "bars1 = ax1.barh(model_names, r2_scores, color='skyblue')\n",
    "ax1.set_xlabel('R¬≤ Score')\n",
    "ax1.set_title('Test R¬≤ Score Comparison')\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars1, r2_scores):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score:.3f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 2. MAE Comparison\n",
    "ax2 = axes[0, 1]\n",
    "mae_scores = results_df['Test_MAE'].values\n",
    "\n",
    "bars2 = ax2.barh(model_names, mae_scores, color='lightcoral')\n",
    "ax2.set_xlabel('MAE (‚Çπ lakhs)')\n",
    "ax2.set_title('Test MAE Comparison')\n",
    "\n",
    "for bar, score in zip(bars2, mae_scores):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score:.1f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 3. Cross-validation vs Test Performance\n",
    "ax3 = axes[0, 2]\n",
    "cv_scores = results_df['CV_R2_Mean'].values\n",
    "test_scores = results_df['Test_R2'].values\n",
    "\n",
    "ax3.scatter(cv_scores, test_scores, s=100, alpha=0.7)\n",
    "ax3.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect Agreement')\n",
    "ax3.set_xlabel('CV R¬≤ Score')\n",
    "ax3.set_ylabel('Test R¬≤ Score')\n",
    "ax3.set_title('CV vs Test Performance')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add model labels\n",
    "for i, name in enumerate(model_names):\n",
    "    ax3.annotate(name[:10], (cv_scores[i], test_scores[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 4. Predictions vs Actual (Best Model)\n",
    "ax4 = axes[1, 0]\n",
    "best_predictions = best_model_results['predictions']\n",
    "\n",
    "ax4.scatter(y_test, best_predictions, alpha=0.6, s=50)\n",
    "min_price = min(y_test.min(), best_predictions.min())\n",
    "max_price = max(y_test.max(), best_predictions.max())\n",
    "ax4.plot([min_price, max_price], [min_price, max_price], 'r--', lw=2, \n",
    "         label='Perfect Prediction')\n",
    "\n",
    "ax4.set_xlabel('Actual Price (‚Çπ lakhs)')\n",
    "ax4.set_ylabel('Predicted Price (‚Çπ lakhs)')\n",
    "ax4.set_title(f'Predictions vs Actual - {best_model_name}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residuals Plot (Best Model)\n",
    "ax5 = axes[1, 1]\n",
    "residuals = best_model_results['residuals']\n",
    "\n",
    "ax5.scatter(best_predictions, residuals, alpha=0.6, s=50)\n",
    "ax5.axhline(y=0, color='red', linestyle='--', lw=2)\n",
    "ax5.set_xlabel('Predicted Price (‚Çπ lakhs)')\n",
    "ax5.set_ylabel('Residuals (‚Çπ lakhs)')\n",
    "ax5.set_title(f'Residual Plot - {best_model_name}')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Overfitting Analysis\n",
    "ax6 = axes[1, 2]\n",
    "overfitting_scores = results_df['Overfitting'].values\n",
    "\n",
    "colors = ['red' if x > 0.1 else 'orange' if x > 0.05 else 'green' for x in overfitting_scores]\n",
    "bars6 = ax6.barh(model_names, overfitting_scores, color=colors)\n",
    "ax6.set_xlabel('Overfitting (Train R¬≤ - Test R¬≤)')\n",
    "ax6.set_title('Overfitting Analysis')\n",
    "ax6.axvline(x=0.05, color='orange', linestyle='--', alpha=0.7, label='Moderate')\n",
    "ax6.axvline(x=0.1, color='red', linestyle='--', alpha=0.7, label='High')\n",
    "ax6.legend()\n",
    "\n",
    "for bar, score in zip(bars6, overfitting_scores):\n",
    "    width = bar.get_width()\n",
    "    ax6.text(width + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score:.3f}', ha='left', va='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "print(\"üå≥ Feature Importance Analysis...\")\n",
    "\n",
    "# Get feature importance from tree-based models\n",
    "importance_results = {}\n",
    "\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Extra Trees', 'Gradient Boosting']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in test_results and test_results[model_name] is not None:\n",
    "        model = test_results[model_name]['model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_results[model_name] = dict(zip(\n",
    "                X_train.columns, \n",
    "                model.feature_importances_\n",
    "            ))\n",
    "\n",
    "if importance_results:\n",
    "    # Create feature importance comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Feature Importance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model_name, importance) in enumerate(importance_results.items()):\n",
    "        if idx < 4:\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            features = list(importance.keys())\n",
    "            importances = list(importance.values())\n",
    "            \n",
    "            # Sort by importance\n",
    "            sorted_idx = np.argsort(importances)[::-1]\n",
    "            features_sorted = [features[i] for i in sorted_idx]\n",
    "            importances_sorted = [importances[i] for i in sorted_idx]\n",
    "            \n",
    "            bars = ax.barh(features_sorted, importances_sorted, color='lightblue')\n",
    "            ax.set_title(f'{model_name}')\n",
    "            ax.set_xlabel('Feature Importance')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, imp in zip(bars, importances_sorted):\n",
    "                width = bar.get_width()\n",
    "                ax.text(width + max(importances_sorted)*0.01, \n",
    "                       bar.get_y() + bar.get_height()/2,\n",
    "                       f'{imp:.3f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(importance_results), 4):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print feature importance summary\n",
    "    print(f\"\\nüìä Feature Importance Summary:\")\n",
    "    \n",
    "    # Average importance across all tree models\n",
    "    all_features = set()\n",
    "    for importance in importance_results.values():\n",
    "        all_features.update(importance.keys())\n",
    "    \n",
    "    avg_importance = {}\n",
    "    for feature in all_features:\n",
    "        importances = [importance.get(feature, 0) for importance in importance_results.values()]\n",
    "        avg_importance[feature] = np.mean(importances)\n",
    "    \n",
    "    # Sort by average importance\n",
    "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Average feature importance across tree-based models:\")\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"   {feature}: {importance:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No tree-based models available for feature importance analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99296b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for top performing models\n",
    "print(\"üéõÔ∏è Hyperparameter Tuning for Top Models...\")\n",
    "\n",
    "# Select top 3 models for tuning\n",
    "top_models = results_df.head(3)['Model'].tolist()\n",
    "print(f\"Tuning hyperparameters for: {top_models}\")\n",
    "\n",
    "tuned_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    print(f\"\\nüîß Tuning {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        if model_name == 'Random Forest':\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "            \n",
    "        elif model_name == 'Gradient Boosting':\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "            base_model = GradientBoostingRegressor(random_state=42)\n",
    "            \n",
    "        elif model_name == 'Ridge Regression':\n",
    "            param_grid = {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "            }\n",
    "            base_model = Ridge(random_state=42)\n",
    "            \n",
    "        elif model_name == 'Extra Trees':\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            base_model = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No tuning configuration for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Use RandomizedSearchCV for efficiency\n",
    "        random_search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,  # Number of parameter settings sampled\n",
    "            cv=3,  # 3-fold CV for speed\n",
    "            scoring='r2',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit the random search\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate best model\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        tuned_train_r2 = r2_score(y_train, y_train_pred)\n",
    "        tuned_test_r2 = r2_score(y_test, y_test_pred)\n",
    "        tuned_test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        # Compare with original\n",
    "        original_test_r2 = test_results[model_name]['test_r2']\n",
    "        improvement = tuned_test_r2 - original_test_r2\n",
    "        \n",
    "        tuned_results[model_name] = {\n",
    "            'best_model': best_model,\n",
    "            'best_params': random_search.best_params_,\n",
    "            'best_cv_score': random_search.best_score_,\n",
    "            'tuned_train_r2': tuned_train_r2,\n",
    "            'tuned_test_r2': tuned_test_r2,\n",
    "            'tuned_test_mae': tuned_test_mae,\n",
    "            'improvement': improvement,\n",
    "            'predictions': y_test_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"   Best parameters: {random_search.best_params_}\")\n",
    "        print(f\"   Original Test R¬≤: {original_test_r2:.4f}\")\n",
    "        print(f\"   Tuned Test R¬≤: {tuned_test_r2:.4f}\")\n",
    "        print(f\"   Improvement: {improvement:.4f}\")\n",
    "        \n",
    "        if improvement > 0.01:\n",
    "            print(f\"   ‚úÖ Significant improvement achieved!\")\n",
    "        else:\n",
    "            print(f\"   ‚û°Ô∏è Marginal improvement\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Tuning failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Hyperparameter tuning completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection and saving\n",
    "print(\"üíæ Final Model Selection and Saving...\")\n",
    "\n",
    "# Determine the best model (considering tuned versions)\n",
    "best_final_model = None\n",
    "best_final_score = -np.inf\n",
    "best_final_name = None\n",
    "\n",
    "# Check original models\n",
    "for name, results in test_results.items():\n",
    "    if results is not None and results['test_r2'] > best_final_score:\n",
    "        best_final_score = results['test_r2']\n",
    "        best_final_model = results['model']\n",
    "        best_final_name = name\n",
    "\n",
    "# Check tuned models\n",
    "for name, results in tuned_results.items():\n",
    "    if results['tuned_test_r2'] > best_final_score:\n",
    "        best_final_score = results['tuned_test_r2']\n",
    "        best_final_model = results['best_model']\n",
    "        best_final_name = f\"{name} (Tuned)\"\n",
    "\n",
    "print(f\"\\nüèÜ Final Best Model: {best_final_name}\")\n",
    "print(f\"   Test R¬≤ Score: {best_final_score:.4f}\")\n",
    "\n",
    "# Save the best model and related components\n",
    "models_dir = Path('../models/trained_models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_final_model, models_dir / 'best_model.pkl')\n",
    "print(f\"‚úÖ Best model saved: {models_dir / 'best_model.pkl'}\")\n",
    "\n",
    "# Save scaler if used\n",
    "if best_final_name in ['Support Vector Regression', 'K-Nearest Neighbors']:\n",
    "    joblib.dump(scaler, models_dir / 'feature_scaler.pkl')\n",
    "    print(f\"‚úÖ Feature scaler saved: {models_dir / 'feature_scaler.pkl'}\")\n",
    "\n",
    "# Save location encoder if used\n",
    "if 'location_encoded' in X_train.columns:\n",
    "    try:\n",
    "        joblib.dump(le_location, models_dir / 'location_encoder.pkl')\n",
    "        print(f\"‚úÖ Location encoder saved: {models_dir / 'location_encoder.pkl'}\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Location encoder not available to save\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model_name': best_final_name,\n",
    "    'best_model_type': type(best_final_model).__name__,\n",
    "    'test_r2_score': best_final_score,\n",
    "    'feature_columns': list(X_train.columns),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_size': {\n",
    "        'train': X_train.shape[0],\n",
    "        'test': X_test.shape[0],\n",
    "        'features': X_train.shape[1]\n",
    "    },\n",
    "    'model_comparison': results_df.to_dict('records'),\n",
    "    'hyperparameter_tuning': {name: results['best_params'] for name, results in tuned_results.items()}\n",
    "}\n",
    "\n",
    "joblib.dump(metadata, models_dir / 'model_metadata.pkl')\n",
    "print(f\"‚úÖ Model metadata saved: {models_dir / 'model_metadata.pkl'}\")\n",
    "\n",
    "print(f\"\\nüéâ Model experiments completed successfully!\")\n",
    "print(f\"üìä Experiment Summary:\")\n",
    "print(f\"   Models tested: {len(models)}\")\n",
    "print(f\"   Models successfully trained: {len([r for r in test_results.values() if r is not None])}\")\n",
    "print(f\"   Models hyperparameter tuned: {len(tuned_results)}\")\n",
    "print(f\"   Best model: {best_final_name}\")\n",
    "print(f\"   Best R¬≤ score: {best_final_score:.4f}\")\n",
    "print(f\"\\n‚úÖ Ready for deployment!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final performance summary\n",
    "print(\"üìã Final Model Performance Summary\")\n",
    "\n",
    "# Create comprehensive comparison including tuned models\n",
    "final_comparison = []\n",
    "\n",
    "# Add original models\n",
    "for name, results in test_results.items():\n",
    "    if results is not None:\n",
    "        final_comparison.append({\n",
    "            'Model': name,\n",
    "            'Type': 'Original',\n",
    "            'Test_R2': results['test_r2'],\n",
    "            'Test_MAE': results['test_mae'],\n",
    "            'Test_RMSE': results['test_rmse'],\n",
    "            'CV_R2_Mean': results['cv_r2_mean'],\n",
    "            'Overfitting': results['train_r2'] - results['test_r2']\n",
    "        })\n",
    "\n",
    "# Add tuned models\n",
    "for name, results in tuned_results.items():\n",
    "    final_comparison.append({\n",
    "        'Model': f\"{name} (Tuned)\",\n",
    "        'Type': 'Tuned',\n",
    "        'Test_R2': results['tuned_test_r2'],\n",
    "        'Test_MAE': results['tuned_test_mae'],\n",
    "        'Test_RMSE': np.sqrt(mean_squared_error(y_test, results['predictions'])),\n",
    "        'CV_R2_Mean': results['best_cv_score'],\n",
    "        'Overfitting': results['tuned_train_r2'] - results['tuned_test_r2']\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_comparison)\n",
    "final_df = final_df.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Final Model Rankings:\")\n",
    "display(final_df.round(4))\n",
    "\n",
    "# Performance insights\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Best performing model: {final_df.iloc[0]['Model']}\")\n",
    "print(f\"   ‚Ä¢ Highest R¬≤ score: {final_df.iloc[0]['Test_R2']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Lowest MAE: {final_df['Test_MAE'].min():.2f} lakhs\")\n",
    "\n",
    "# Model type performance\n",
    "model_type_performance = final_df.groupby('Type')['Test_R2'].agg(['mean', 'max', 'count'])\n",
    "print(f\"\\nüìä Performance by Model Type:\")\n",
    "display(model_type_performance.round(4))\n",
    "\n",
    "if len(tuned_results) > 0:\n",
    "    avg_improvement = np.mean([r['improvement'] for r in tuned_results.values()])\n",
    "    print(f\"\\nüéõÔ∏è Hyperparameter Tuning Impact:\")\n",
    "    print(f\"   Average R¬≤ improvement: {avg_improvement:.4f}\")\n",
    "    \n",
    "    significant_improvements = sum(1 for r in tuned_results.values() if r['improvement'] > 0.01)\n",
    "    print(f\"   Models with significant improvement: {significant_improvements}/{len(tuned_results)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model experiments analysis completed at: {datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a88c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34db1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
