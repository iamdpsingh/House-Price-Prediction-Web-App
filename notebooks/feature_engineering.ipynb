{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß Feature Engineering Pipeline Started\")\n",
    "print(f\"üìÖ Timestamp: {pd.Timestamp.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ce7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "try:\n",
    "    df_raw = pd.read_csv('../data/raw/bengaluru_house_prices.csv')\n",
    "    print(\"‚úÖ Raw data loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Creating sample data...\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    sample_data = {\n",
    "        'area_type': np.random.choice(['Super built-up Area', 'Built-up Area', 'Plot Area'], 1000),\n",
    "        'availability': np.random.choice(['Ready To Move', '18-Jun', '19-Dec'], 1000, p=[0.7, 0.2, 0.1]),\n",
    "        'location': np.random.choice(['Whitefield', 'Electronic City', 'Marathahalli', 'BTM Layout', 'Koramangala'], 1000),\n",
    "        'size': [f\"{np.random.choice([1,2,3,4])} BHK\" for _ in range(1000)],\n",
    "        'society': [f\"Society {i}\" for i in np.random.randint(1, 100, 1000)],\n",
    "        'total_sqft': np.random.normal(1200, 400, 1000),\n",
    "        'bath': np.random.randint(1, 5, 1000),\n",
    "        'balcony': np.random.randint(0, 4, 1000),\n",
    "        'price': np.random.normal(80, 30, 1000)\n",
    "    }\n",
    "    df_raw = pd.DataFrame(sample_data)\n",
    "    df_raw['total_sqft'] = np.maximum(df_raw['total_sqft'], 500)\n",
    "    df_raw['price'] = np.maximum(df_raw['price'], 20)\n",
    "    print(\"üìä Sample data created\")\n",
    "\n",
    "print(f\"\\nüìã Dataset Info:\")\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display sample\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data quality assessment\n",
    "print(\"üîç Initial Data Quality Assessment:\")\n",
    "print(f\"Missing values: {df_raw.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df_raw.duplicated().sum()}\")\n",
    "\n",
    "# Missing values by column\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df_raw.columns,\n",
    "    'Missing_Count': df_raw.isnull().sum(),\n",
    "    'Missing_Percent': (df_raw.isnull().sum() / len(df_raw)) * 100,\n",
    "    'Data_Type': df_raw.dtypes\n",
    "})\n",
    "\n",
    "display(missing_summary[missing_summary['Missing_Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Data cleaning pipeline for house price dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.cleaning_log = []\n",
    "        \n",
    "    def log_step(self, step, before_shape, after_shape, description):\n",
    "        \"\"\"Log cleaning steps\"\"\"\n",
    "        rows_removed = before_shape[0] - after_shape[0]\n",
    "        self.cleaning_log.append({\n",
    "            'Step': step,\n",
    "            'Description': description,\n",
    "            'Rows_Before': before_shape[0],\n",
    "            'Rows_After': after_shape[0],\n",
    "            'Rows_Removed': rows_removed,\n",
    "            'Percent_Removed': (rows_removed / before_shape[0]) * 100\n",
    "        })\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate rows\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Remove Duplicates', before_shape, after_shape, \n",
    "                     'Removed duplicate rows')\n",
    "        return self\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing values based on column type and amount\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        \n",
    "        # For this dataset, we'll drop rows with missing critical values\n",
    "        critical_columns = ['location', 'total_sqft', 'price']\n",
    "        self.df = self.df.dropna(subset=critical_columns)\n",
    "        \n",
    "        # Fill missing balcony with 0 (reasonable default)\n",
    "        if 'balcony' in self.df.columns:\n",
    "            self.df['balcony'] = self.df['balcony'].fillna(0)\n",
    "        \n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Handle Missing Values', before_shape, after_shape,\n",
    "                     'Dropped rows with missing critical values')\n",
    "        return self\n",
    "    \n",
    "    def clean_total_sqft(self):\n",
    "        \"\"\"Clean and standardize total_sqft column\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        \n",
    "        def clean_sqft_value(value):\n",
    "            if pd.isna(value):\n",
    "                return np.nan\n",
    "            \n",
    "            # Convert to string and clean\n",
    "            value_str = str(value).strip()\n",
    "            \n",
    "            # Handle range values like \"1200 - 1300\"\n",
    "            if '-' in value_str:\n",
    "                try:\n",
    "                    parts = value_str.split('-')\n",
    "                    if len(parts) == 2:\n",
    "                        lower = float(parts[0].strip())\n",
    "                        upper = float(parts[1].strip())\n",
    "                        return (lower + upper) / 2\n",
    "                except:\n",
    "                    return np.nan\n",
    "            \n",
    "            # Handle single values\n",
    "            try:\n",
    "                # Remove any non-numeric characters except decimal point\n",
    "                cleaned_value = re.sub(r'[^\\d.]', '', value_str)\n",
    "                return float(cleaned_value) if cleaned_value else np.nan\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        self.df['total_sqft'] = self.df['total_sqft'].apply(clean_sqft_value)\n",
    "        \n",
    "        # Remove rows with invalid sqft values\n",
    "        self.df = self.df.dropna(subset=['total_sqft'])\n",
    "        \n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Clean Total Sqft', before_shape, after_shape,\n",
    "                     'Cleaned and standardized sqft values')\n",
    "        return self\n",
    "    \n",
    "    def extract_bhk(self):\n",
    "        \"\"\"Extract BHK number from size column\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        \n",
    "        def extract_bhk_number(size_str):\n",
    "            if pd.isna(size_str):\n",
    "                return np.nan\n",
    "            \n",
    "            # Extract number from strings like \"2 BHK\", \"3BHK\", \"4 Bedroom\"\n",
    "            size_str = str(size_str).strip().upper()\n",
    "            \n",
    "            # Find numbers in the string\n",
    "            numbers = re.findall(r'\\d+', size_str)\n",
    "            \n",
    "            if numbers:\n",
    "                bhk_num = int(numbers[0])\n",
    "                # Reasonable bounds for BHK\n",
    "                return bhk_num if 1 <= bhk_num <= 10 else np.nan\n",
    "            \n",
    "            return np.nan\n",
    "        \n",
    "        self.df['bhk'] = self.df['size'].apply(extract_bhk_number)\n",
    "        \n",
    "        # Remove rows where BHK extraction failed\n",
    "        self.df = self.df.dropna(subset=['bhk'])\n",
    "        \n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Extract BHK', before_shape, after_shape,\n",
    "                     'Extracted BHK numbers from size column')\n",
    "        return self\n",
    "    \n",
    "    def clean_location(self):\n",
    "        \"\"\"Clean and standardize location names\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        \n",
    "        def clean_location_name(location):\n",
    "            if pd.isna(location):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            location = str(location).strip()\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            location = re.sub(r'\\s+', ' ', location)\n",
    "            \n",
    "            # Title case\n",
    "            location = location.title()\n",
    "            \n",
    "            # Common standardizations\n",
    "            standardizations = {\n",
    "                'Electronic City': 'Electronic City',\n",
    "                'E City': 'Electronic City',\n",
    "                'Ecity': 'Electronic City',\n",
    "                'Whitefield': 'Whitefield',\n",
    "                'White Field': 'Whitefield',\n",
    "                'Sarjapur Road': 'Sarjapur Road',\n",
    "                'Sarjapura Road': 'Sarjapur Road',\n",
    "            }\n",
    "            \n",
    "            for variant, standard in standardizations.items():\n",
    "                if location.lower() == variant.lower():\n",
    "                    return standard\n",
    "            \n",
    "            return location\n",
    "        \n",
    "        self.df['location'] = self.df['location'].apply(clean_location_name)\n",
    "        \n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Clean Location', before_shape, after_shape,\n",
    "                     'Cleaned and standardized location names')\n",
    "        return self\n",
    "    \n",
    "    def remove_outliers(self, method='iqr', multiplier=1.5):\n",
    "        \"\"\"Remove outliers using specified method\"\"\"\n",
    "        before_shape = self.df.shape\n",
    "        \n",
    "        numerical_cols = ['price', 'total_sqft', 'bhk', 'bath']\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            if col in self.df.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.df[col].quantile(0.25)\n",
    "                    Q3 = self.df[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - multiplier * IQR\n",
    "                    upper_bound = Q3 + multiplier * IQR\n",
    "                    \n",
    "                    self.df = self.df[\n",
    "                        (self.df[col] >= lower_bound) & \n",
    "                        (self.df[col] <= upper_bound)\n",
    "                    ]\n",
    "        \n",
    "        # Business logic filtering\n",
    "        if 'bath' in self.df.columns and 'bhk' in self.df.columns:\n",
    "            # Bathrooms should not exceed BHK by more than 2\n",
    "            self.df = self.df[self.df['bath'] <= self.df['bhk'] + 2]\n",
    "        \n",
    "        if 'total_sqft' in self.df.columns and 'bhk' in self.df.columns:\n",
    "            # Minimum 300 sqft per room\n",
    "            self.df['sqft_per_room'] = self.df['total_sqft'] / self.df['bhk']\n",
    "            self.df = self.df[self.df['sqft_per_room'] >= 300]\n",
    "        \n",
    "        after_shape = self.df.shape\n",
    "        self.log_step('Remove Outliers', before_shape, after_shape,\n",
    "                     f'Removed outliers using {method} method')\n",
    "        return self\n",
    "    \n",
    "    def get_cleaned_data(self):\n",
    "        \"\"\"Return cleaned dataset and cleaning log\"\"\"\n",
    "        return self.df, pd.DataFrame(self.cleaning_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning pipeline\n",
    "print(\"üßπ Applying Data Cleaning Pipeline...\")\n",
    "cleaner = DataCleaner(df_raw)\n",
    "\n",
    "df_cleaned, cleaning_log = (cleaner\n",
    "                           .remove_duplicates()\n",
    "                           .handle_missing_values()\n",
    "                           .clean_total_sqft()\n",
    "                           .extract_bhk()\n",
    "                           .clean_location()\n",
    "                           .remove_outliers()\n",
    "                           .get_cleaned_data())\n",
    "\n",
    "print(\"\\nüìä Cleaning Summary:\")\n",
    "display(cleaning_log)\n",
    "\n",
    "print(f\"\\nüìà Final Dataset:\")\n",
    "print(f\"Original shape: {df_raw.shape}\")\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "print(f\"Data retention: {len(df_cleaned)/len(df_raw)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32539f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Feature engineering pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.feature_log = []\n",
    "        \n",
    "    def log_feature(self, feature_name, description):\n",
    "        \"\"\"Log feature creation\"\"\"\n",
    "        self.feature_log.append({\n",
    "            'Feature': feature_name,\n",
    "            'Description': description,\n",
    "            'Type': 'Derived'\n",
    "        })\n",
    "    \n",
    "    def create_price_features(self):\n",
    "        \"\"\"Create price-related features\"\"\"\n",
    "        if 'price' in self.df.columns and 'total_sqft' in self.df.columns:\n",
    "            # Price per square feet\n",
    "            self.df['price_per_sqft'] = (self.df['price'] * 100000) / self.df['total_sqft']\n",
    "            self.log_feature('price_per_sqft', 'Price per square feet in rupees')\n",
    "            \n",
    "            # Price category\n",
    "            self.df['price_category'] = pd.cut(\n",
    "                self.df['price'],\n",
    "                bins=[0, 30, 60, 100, 200, float('inf')],\n",
    "                labels=['Budget', 'Mid-range', 'Premium', 'Luxury', 'Ultra-luxury']\n",
    "            )\n",
    "            self.log_feature('price_category', 'Categorical price segments')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_size_features(self):\n",
    "        \"\"\"Create size-related features\"\"\"\n",
    "        if 'total_sqft' in self.df.columns and 'bhk' in self.df.columns:\n",
    "            # Square feet per room\n",
    "            self.df['sqft_per_room'] = self.df['total_sqft'] / self.df['bhk']\n",
    "            self.log_feature('sqft_per_room', 'Square feet per room (room size)')\n",
    "        \n",
    "        if 'bath' in self.df.columns and 'bhk' in self.df.columns:\n",
    "            # Bath to BHK ratio\n",
    "            self.df['bath_per_bhk'] = self.df['bath'] / self.df['bhk']\n",
    "            self.log_feature('bath_per_bhk', 'Bathroom to bedroom ratio')\n",
    "        \n",
    "        # Total rooms (including living areas)\n",
    "        room_cols = ['bhk', 'bath']\n",
    "        if 'balcony' in self.df.columns:\n",
    "            room_cols.append('balcony')\n",
    "        \n",
    "        if all(col in self.df.columns for col in room_cols):\n",
    "            self.df['total_rooms'] = self.df[room_cols].sum(axis=1)\n",
    "            self.log_feature('total_rooms', 'Total number of rooms')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_location_features(self):\n",
    "        \"\"\"Create location-based features\"\"\"\n",
    "        if 'location' in self.df.columns:\n",
    "            # Location frequency (popularity)\n",
    "            location_counts = self.df['location'].value_counts()\n",
    "            self.df['location_frequency'] = self.df['location'].map(location_counts)\n",
    "            self.log_feature('location_frequency', 'Number of properties in same location')\n",
    "            \n",
    "            # Location tier based on average price\n",
    "            if 'price' in self.df.columns:\n",
    "                location_avg_price = self.df.groupby('location')['price'].mean()\n",
    "                price_quartiles = location_avg_price.quantile([0.33, 0.67])\n",
    "                \n",
    "                def get_location_tier(location):\n",
    "                    avg_price = location_avg_price.get(location, 0)\n",
    "                    if avg_price >= price_quartiles[0.67]:\n",
    "                        return 'Premium'\n",
    "                    elif avg_price >= price_quartiles[0.33]:\n",
    "                        return 'Mid-tier'\n",
    "                    else:\n",
    "                        return 'Budget'\n",
    "                \n",
    "                self.df['location_tier'] = self.df['location'].apply(get_location_tier)\n",
    "                self.log_feature('location_tier', 'Location tier based on average price')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_amenity_features(self):\n",
    "        \"\"\"Create amenity-related features\"\"\"\n",
    "        # Balcony indicator\n",
    "        if 'balcony' in self.df.columns:\n",
    "            self.df['has_balcony'] = (self.df['balcony'] > 0).astype(int)\n",
    "            self.log_feature('has_balcony', 'Binary indicator for balcony presence')\n",
    "        \n",
    "        # Luxury indicator (multiple criteria)\n",
    "        luxury_conditions = []\n",
    "        \n",
    "        if 'bhk' in self.df.columns:\n",
    "            luxury_conditions.append(self.df['bhk'] >= 4)\n",
    "        \n",
    "        if 'bath' in self.df.columns:\n",
    "            luxury_conditions.append(self.df['bath'] >= 3)\n",
    "        \n",
    "        if 'total_sqft' in self.df.columns:\n",
    "            luxury_conditions.append(self.df['total_sqft'] >= 2000)\n",
    "        \n",
    "        if luxury_conditions:\n",
    "            # Property is luxury if it meets at least 2 criteria\n",
    "            luxury_score = sum(luxury_conditions)\n",
    "            self.df['is_luxury'] = (luxury_score >= 2).astype(int)\n",
    "            self.log_feature('is_luxury', 'Luxury property indicator')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_efficiency_features(self):\n",
    "        \"\"\"Create efficiency and value features\"\"\"\n",
    "        if 'price_per_sqft' in self.df.columns:\n",
    "            # Price efficiency relative to location average\n",
    "            if 'location' in self.df.columns:\n",
    "                location_avg_psqft = self.df.groupby('location')['price_per_sqft'].mean()\n",
    "                self.df['price_efficiency'] = (\n",
    "                    self.df['price_per_sqft'] / \n",
    "                    self.df['location'].map(location_avg_psqft)\n",
    "                )\n",
    "                self.log_feature('price_efficiency', 'Price relative to location average')\n",
    "        \n",
    "        # Space efficiency\n",
    "        if 'total_sqft' in self.df.columns and 'total_rooms' in self.df.columns:\n",
    "            self.df['space_efficiency'] = self.df['total_sqft'] / self.df['total_rooms']\n",
    "            self.log_feature('space_efficiency', 'Square feet per total room')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_engineered_data(self):\n",
    "        \"\"\"Return feature-engineered dataset and feature log\"\"\"\n",
    "        return self.df, pd.DataFrame(self.feature_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f16f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"‚öôÔ∏è Applying Feature Engineering Pipeline...\")\n",
    "engineer = FeatureEngineer(df_cleaned)\n",
    "\n",
    "df_featured, feature_log = (engineer\n",
    "                           .create_price_features()\n",
    "                           .create_size_features()\n",
    "                           .create_location_features()\n",
    "                           .create_amenity_features()\n",
    "                           .create_efficiency_features()\n",
    "                           .get_engineered_data())\n",
    "\n",
    "print(\"\\nüìä Feature Engineering Summary:\")\n",
    "display(feature_log)\n",
    "\n",
    "print(f\"\\nüìà Feature Engineering Results:\")\n",
    "print(f\"Original features: {len(df_cleaned.columns)}\")\n",
    "print(f\"Final features: {len(df_featured.columns)}\")\n",
    "print(f\"New features created: {len(df_featured.columns) - len(df_cleaned.columns)}\")\n",
    "\n",
    "# Display sample of new features\n",
    "new_features = [col for col in df_featured.columns if col not in df_cleaned.columns]\n",
    "if new_features:\n",
    "    print(f\"\\nüÜï New Features Sample:\")\n",
    "    display(df_featured[new_features].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoding pipeline\n",
    "print(\"üè∑Ô∏è Categorical Encoding Pipeline\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_featured.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Initialize encoders\n",
    "encoders = {}\n",
    "df_encoded = df_featured.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"\\nEncoding {col}:\")\n",
    "        print(f\"  Unique values: {df_encoded[col].nunique()}\")\n",
    "        \n",
    "        if df_encoded[col].nunique() <= 20:  # Use label encoding for reasonable number of categories\n",
    "            # Label Encoding\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[f'{col}_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
    "            encoders[col] = le\n",
    "            \n",
    "            print(f\"  Applied Label Encoding\")\n",
    "            print(f\"  Encoded range: 0 to {len(le.classes_) - 1}\")\n",
    "            \n",
    "            # Show encoding mapping for small sets\n",
    "            if len(le.classes_) <= 10:\n",
    "                encoding_map = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "                print(f\"  Encoding map: {encoding_map}\")\n",
    "        \n",
    "        else:\n",
    "            # For high cardinality, use frequency encoding\n",
    "            freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "            df_encoded[f'{col}_frequency'] = df_encoded[col].map(freq_encoding)\n",
    "            encoders[f'{col}_frequency'] = freq_encoding\n",
    "            \n",
    "            print(f\"  Applied Frequency Encoding (high cardinality)\")\n",
    "            print(f\"  Frequency range: {df_encoded[f'{col}_frequency'].min()} to {df_encoded[f'{col}_frequency'].max()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Categorical encoding completed\")\n",
    "print(f\"Encoders created: {len(encoders)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabba462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling\n",
    "print(\"üéØ Feature Selection for Modeling\")\n",
    "\n",
    "# Define features for modeling\n",
    "modeling_features = [\n",
    "    # Core features\n",
    "    'total_sqft', 'bhk', 'bath', 'balcony',\n",
    "    \n",
    "    # Encoded categorical features\n",
    "    'location_encoded',\n",
    "    \n",
    "    # Derived features\n",
    "    'price_per_sqft', 'sqft_per_room', 'bath_per_bhk',\n",
    "    'location_frequency', 'has_balcony'\n",
    "]\n",
    "\n",
    "# Select features that exist in the dataset\n",
    "available_features = [col for col in modeling_features if col in df_encoded.columns]\n",
    "missing_features = [col for col in modeling_features if col not in df_encoded.columns]\n",
    "\n",
    "print(f\"Available features ({len(available_features)}): {available_features}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features ({len(missing_features)}): {missing_features}\")\n",
    "\n",
    "# Create final feature matrix\n",
    "X = df_encoded[available_features].copy()\n",
    "y = df_encoded['price'].copy() if 'price' in df_encoded.columns else None\n",
    "\n",
    "print(f\"\\nüìä Final Feature Matrix:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "if y is not None:\n",
    "    print(f\"Target (y): {y.shape}\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nüìà Feature Statistics:\")\n",
    "display(X.describe().round(2))\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_in_features = X.isnull().sum()\n",
    "if missing_in_features.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing values in features:\")\n",
    "    print(missing_in_features[missing_in_features > 0])\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"‚úÖ Filled missing values with median\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final feature matrix prepared successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaling and train-test split\n",
    "print(\"‚öñÔ∏è Data Scaling and Train-Test Split\")\n",
    "\n",
    "if y is not None:\n",
    "    # Train-test split with stratification\n",
    "    try:\n",
    "        price_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=price_bins\n",
    "        )\n",
    "        print(\"‚úÖ Stratified train-test split completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Stratified split failed: {e}\")\n",
    "        print(\"Using random split instead\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTrain set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrames for easier handling\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature scaling completed\")\n",
    "    print(f\"Scaler fitted on {X_train.shape[0]} training samples\")\n",
    "    \n",
    "    # Scaling statistics\n",
    "    print(f\"\\nüìä Scaling Statistics:\")\n",
    "    scaling_stats = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Original_Mean': X_train.mean(),\n",
    "        'Original_Std': X_train.std(),\n",
    "        'Scaled_Mean': X_train_scaled.mean(),\n",
    "        'Scaled_Std': X_train_scaled.std()\n",
    "    }).round(3)\n",
    "    \n",
    "    display(scaling_stats.head(10))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No target variable found, skipping train-test split\")\n",
    "    X_train, X_test, y_train, y_test = X, None, None, None\n",
    "    \n",
    "    # Still apply scaling to full dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    print(\"‚úÖ Applied scaling to full dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b658dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"üíæ Saving Processed Data\")\n",
    "\n",
    "# Create processed data directory\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save cleaned and featured dataset\n",
    "df_featured.to_csv(processed_dir / 'processed_data.csv', index=False)\n",
    "print(f\"‚úÖ Processed data saved: {processed_dir / 'processed_data.csv'}\")\n",
    "\n",
    "# Save feature matrix\n",
    "if y is not None:\n",
    "    # Save train-test splits\n",
    "    X_train.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "    X_test.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "    y_train.to_csv(processed_dir / 'y_train.csv', index=False, header=['price'])\n",
    "    y_test.to_csv(processed_dir / 'y_test.csv', index=False, header=['price'])\n",
    "    \n",
    "    # Save scaled versions\n",
    "    X_train_scaled.to_csv(processed_dir / 'X_train_scaled.csv', index=False)\n",
    "    X_test_scaled.to_csv(processed_dir / 'X_test_scaled.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Train-test splits saved\")\n",
    "    print(f\"‚úÖ Scaled features saved\")\n",
    "\n",
    "# Save encoders and scaler using joblib\n",
    "import joblib\n",
    "\n",
    "models_dir = Path('../models/trained_models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save encoders\n",
    "if encoders:\n",
    "    joblib.dump(encoders, models_dir / 'encoders.pkl')\n",
    "    print(f\"‚úÖ Encoders saved: {models_dir / 'encoders.pkl'}\")\n",
    "\n",
    "# Save scaler\n",
    "if 'scaler' in locals():\n",
    "    joblib.dump(scaler, models_dir / 'feature_scaler.pkl')\n",
    "    print(f\"‚úÖ Scaler saved: {models_dir / 'feature_scaler.pkl'}\")\n",
    "\n",
    "print(f\"\\nüéâ Feature Engineering Pipeline Completed Successfully!\")\n",
    "print(f\"üìä Final Summary:\")\n",
    "print(f\"   Original records: {len(df_raw):,}\")\n",
    "print(f\"   Final records: {len(df_featured):,}\")\n",
    "print(f\"   Data retention: {len(df_featured)/len(df_raw)*100:.1f}%\")\n",
    "print(f\"   Original features: {len(df_raw.columns)}\")\n",
    "print(f\"   Final features: {len(df_featured.columns)}\")\n",
    "print(f\"   Features for modeling: {len(available_features)}\")\n",
    "print(f\"\\n‚úÖ Ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d0b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62607922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
